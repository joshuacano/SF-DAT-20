{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we are going to use today is **Boston Dataset**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from sklearn import feature_selection\n",
    "import statsmodels.formula.api as smf\n",
    "from sklearn import metrics\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "      <th>medv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.9</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.9</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      crim  zn  indus  chas    nox     rm   age     dis  rad  tax  ptratio  \\\n",
       "0  0.00632  18   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "\n",
       "   black  lstat  medv  \n",
       "0  396.9   4.98  24.0  \n",
       "1  396.9   9.14  21.6  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://raw.githubusercontent.com/ga-students/SF-DAT-20/master/Data/Boston.csv\"\n",
    "BostonData = pd.read_csv(url)\n",
    "del BostonData['Unnamed: 0']\n",
    "BostonData.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Boston data frame has 506 rows and 14 columns.\n",
    "Usage\n",
    "\n",
    "Boston\n",
    "\n",
    "Format\n",
    "\n",
    "This data frame contains the following columns:\n",
    "\n",
    "crim\n",
    "\n",
    "    per capita crime rate by town \n",
    "    \n",
    "zn\n",
    "\n",
    "    proportion of residential land zoned for lots over 25,000 sq.ft. \n",
    "    \n",
    "indus\n",
    "\n",
    "    proportion of non-retail business acres per town \n",
    "    \n",
    "chas\n",
    "\n",
    "    Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) \n",
    "    \n",
    "nox\n",
    "\n",
    "    nitrogen oxides concentration (parts per 10 million) \n",
    "    \n",
    "rm\n",
    "\n",
    "    average number of rooms per dwelling \n",
    "    \n",
    "age\n",
    "\n",
    "    proportion of owner-occupied units built prior to 1940 \n",
    "    \n",
    "dis\n",
    "\n",
    "    weighted mean of distances to five Boston employment centres \n",
    "    \n",
    "rad\n",
    "\n",
    "    index of accessibility to radial highways \n",
    "    \n",
    "tax\n",
    "\n",
    "    full-value property-tax rate per 10,000 dollars\n",
    "    \n",
    "ptratio\n",
    "\n",
    "    pupil-teacher ratio by town \n",
    "    \n",
    "black\n",
    "\n",
    "    1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town \n",
    "    \n",
    "lstat\n",
    "\n",
    "    lower status of the population (percent) \n",
    "    \n",
    "medv\n",
    "\n",
    "    median value of owner-occupied homes in 1000 dollars\n",
    "\n",
    "Source\n",
    "\n",
    "Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. J. Environ. Economics and Management 5, 81â€“102.\n",
    "\n",
    "Belsley D.A., Kuh, E. and Welsch, R.E. (1980) Regression Diagnostics. Identifying Influential Data and Sources of Collinearity. New York: Wiley.\n",
    "[Package MASS version 7.2-29 Index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to predict the median value of properties (medv) based on other variables in this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First let's draw a scatter-plot of medv and lstat. Intuitively, does it like a pure linear association or it seems like there is some sort of non-linearity?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BostonData.plot(kind=\"scatter\", x=\"medv\", y='lstat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: This looks like a curve based association, close to a line but not quite exactly on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, let's first define few non-linear terms. Start from a pure linear function and go up to polynomial degree 5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "BostonData['lstat_3'] = BostonData['lstat']**3\n",
    "BostonData['lstat_4'] = BostonData['lstat']**4\n",
    "BostonData['lstat_5'] = BostonData['lstat']**5\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','lstat_2','lstat_3']]\n",
    "X4 = BostonData[['lstat','lstat_2','lstat_3','lstat_4']]\n",
    "X5 = BostonData[['lstat','lstat_2','lstat_3','lstat_4','lstat_5']]\n",
    "y = BostonData['medv']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now divide your dataset into 25% test set and 75% training set and use Validation and MSE of test set to decide which degree of polynomial fits the best. Run this procedure a few times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = BostonData[\"medv\"]\n",
    "\n",
    "def get_min_index(scores):\n",
    "    if len(scores) < 1:\n",
    "        return -1 \n",
    "    min_score = scores[0]\n",
    "    min_index = 0\n",
    "    for index, score in enumerate(scores):\n",
    "        if score < min_score:\n",
    "            min_index = index\n",
    "            min_score = score\n",
    "    return min_index\n",
    "\n",
    "scores = []\n",
    "lm = linear_model.LinearRegression()\n",
    "list_count = []\n",
    "index = np.array(range(5))+ 1\n",
    "\n",
    "test_x = np.zeros(5)\n",
    "train_x = np.zeros(5)\n",
    "\n",
    "for num in range(9):\n",
    "    for i,x in enumerate([X1, X2, X3, X4, X5]):\n",
    "            X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.25)\n",
    "            lm.fit(X_train, Y_train)\n",
    "            test_score = metrics.mean_squared_error(lm.predict(X_test), Y_test)\n",
    "            train_score = metrics.mean_squared_error(lm.predict(X_train), Y_train)\n",
    "            scores.append(test_score)\n",
    "            test_x[i] = (test_score)\n",
    "            train_x[i] = train_score\n",
    "        #Calculate minimum_score \n",
    "        #print scores\n",
    "    list_count.append(get_min_index(scores))\n",
    "    scores = []\n",
    "    MSE_Test_df = pd.DataFrame({'test_x':test_x,'index':index})\n",
    "    MSE_Test_df.plot(x = 'index',y= 'test_x')\n",
    "\n",
    "print Counter(list_count).most_common(3)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: it seems validation is not a good method to handle this "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, on the same data set, use 10 fold cross-validation to decide on the degree of polynomial. Justify what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "list_count = []\n",
    "#for i in range(10):\n",
    "kf = cross_validation.KFold(len(BostonData), n_folds = 10, shuffle=True)\n",
    "\n",
    "test_x = np.zeros(5)\n",
    "scores = []\n",
    "for i, x in enumerate([X1, X2, X3, X4, X5]):\n",
    "    for train_index, test_index in kf:\n",
    "        lm.fit(x.iloc[train_index], y.iloc[train_index])\n",
    "        err_test = lm.predict(x.iloc[test_index])\n",
    "        current_score = metrics.mean_squared_error(err_test, y.iloc[test_index])\n",
    "        scores.append(current_score)\n",
    "    test_x[i] = np.mean(scores)\n",
    "    #Calculate minimum_score \n",
    "        #print scores\n",
    "    #list_count.append(get_min_index(scores))\n",
    "    scores = []\n",
    "\n",
    "MSE_Test_df = pd.DataFrame({'test_x':test_x,'index':index})\n",
    "MSE_Test_df.plot(x = 'index',y= 'test_x')\n",
    "#print Counter(list_count).most_common(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: IT is much more overwhelmingly the 4th index in this case, degree 2 in this case is very useful as increasing polynomial degree can come with great cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's consider more variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's first focus on correlation Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's first get rid of additional variables we added to our dataframe\n",
    "del BostonData['lstat_2']\n",
    "del BostonData['lstat_3']\n",
    "del BostonData['lstat_4']\n",
    "del BostonData['lstat_5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BostonData.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List 3 variables that have the highest chance to appear in your final model - the model that can predict medv. Can these variables appear simultaneously in your final model if your goal is interpretation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: the main 3 variables will be rm, lstat and ptratio. These variables cannot appear if our goal is interpretation as they are highly correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's standardize our data and put it in a new DataFrame called BostonDataNew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonDataNew = preprocessing.scale(BostonData)\n",
    "BostonDataNew = pd.DataFrame(BostonDataNew)\n",
    "BostonDataNew.columns = BostonData.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation and Lasso regression on our standardized data to decide which variables to eliminate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>crim</th>\n",
       "      <th>zn</th>\n",
       "      <th>indus</th>\n",
       "      <th>chas</th>\n",
       "      <th>nox</th>\n",
       "      <th>rm</th>\n",
       "      <th>age</th>\n",
       "      <th>dis</th>\n",
       "      <th>rad</th>\n",
       "      <th>tax</th>\n",
       "      <th>ptratio</th>\n",
       "      <th>black</th>\n",
       "      <th>lstat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.419782</td>\n",
       "      <td>0.284830</td>\n",
       "      <td>-1.287909</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.144217</td>\n",
       "      <td>0.413672</td>\n",
       "      <td>-0.120013</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>-0.982843</td>\n",
       "      <td>-0.666608</td>\n",
       "      <td>-1.459000</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.075562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.417339</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>0.194274</td>\n",
       "      <td>0.367166</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-0.492439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.417342</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-0.593381</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.740262</td>\n",
       "      <td>1.282714</td>\n",
       "      <td>-0.265812</td>\n",
       "      <td>0.557160</td>\n",
       "      <td>-0.867883</td>\n",
       "      <td>-0.987329</td>\n",
       "      <td>-0.303094</td>\n",
       "      <td>0.396427</td>\n",
       "      <td>-1.208727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.416750</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.016303</td>\n",
       "      <td>-0.809889</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.416163</td>\n",
       "      <td>-1.361517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412482</td>\n",
       "      <td>-0.487722</td>\n",
       "      <td>-1.306878</td>\n",
       "      <td>-0.272599</td>\n",
       "      <td>-0.835284</td>\n",
       "      <td>1.228577</td>\n",
       "      <td>-0.511180</td>\n",
       "      <td>1.077737</td>\n",
       "      <td>-0.752922</td>\n",
       "      <td>-1.106115</td>\n",
       "      <td>0.113032</td>\n",
       "      <td>0.441052</td>\n",
       "      <td>-1.026501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       crim        zn     indus      chas       nox        rm       age  \\\n",
       "0 -0.419782  0.284830 -1.287909 -0.272599 -0.144217  0.413672 -0.120013   \n",
       "1 -0.417339 -0.487722 -0.593381 -0.272599 -0.740262  0.194274  0.367166   \n",
       "2 -0.417342 -0.487722 -0.593381 -0.272599 -0.740262  1.282714 -0.265812   \n",
       "3 -0.416750 -0.487722 -1.306878 -0.272599 -0.835284  1.016303 -0.809889   \n",
       "4 -0.412482 -0.487722 -1.306878 -0.272599 -0.835284  1.228577 -0.511180   \n",
       "\n",
       "        dis       rad       tax   ptratio     black     lstat  \n",
       "0  0.140214 -0.982843 -0.666608 -1.459000  0.441052 -1.075562  \n",
       "1  0.557160 -0.867883 -0.987329 -0.303094  0.441052 -0.492439  \n",
       "2  0.557160 -0.867883 -0.987329 -0.303094  0.396427 -1.208727  \n",
       "3  1.077737 -0.752922 -1.106115  0.113032  0.416163 -1.361517  \n",
       "4  1.077737 -0.752922 -1.106115  0.113032  0.441052 -1.026501  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "listOfAllVariables = BostonData.columns.values\n",
    "X = BostonDataNew[listOfAllVariables]\n",
    "Y = BostonDataNew[\"medv\"]\n",
    "del X[\"medv\"]\n",
    "X.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kf = cross_validation.KFold(len(BostonDataNew), n_folds = 10, shuffle = True)\n",
    "alphas = np.logspace(-10, 10, 21)\n",
    "alpha_index = np.linspace(-10, 10, 21)\n",
    "scores = []\n",
    "MSE_lasso_csv = []\n",
    "for a in alphas:\n",
    "    print \"Alpha: \" , a\n",
    "    scores = []\n",
    "    for train_index, test_index in kf:\n",
    "        lm = linear_model.Lasso(alpha=a).fit(X.iloc[train_index], Y.iloc[train_index])\n",
    "        scores.append(metrics.mean_squared_error(lm.predict(X.iloc[test_index]), Y.iloc[test_index]))\n",
    "    MSE_lasso_csv.append(np.mean(scores))\n",
    "\n",
    "MSE_lasso_csv_df = pd.DataFrame({\"MSE_lasso_csv\" : MSE_lasso_csv, \"Log_Alphas\" : alpha_index})\n",
    "MSE_lasso_csv_df.plot(x =\"Log_Alphas\", y = \"MSE_lasso_csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(-0.071461275127387022, 'crim'), (0.080283910755197502, 'zn'), (-0.0, 'indus'), (0.071857996341448307, 'chas'), (-0.1752046368173589, 'nox'), (0.30621113329030625, 'rm'), (-0.0, 'age'), (-0.26996579836071716, 'dis'), (0.14261427135427748, 'rad'), (-0.10216247651642624, 'tax'), (-0.2103399918606835, 'ptratio'), (0.083704310613461799, 'black'), (-0.40556056135991536, 'lstat')]\n"
     ]
    }
   ],
   "source": [
    "lm = linear_model.Lasso(alpha=10**(-2)) #we could also use -2 but that doesn't give us a clear story.\n",
    "lm.fit(X, Y)\n",
    "print zip(lm.coef_,X.columns)\n",
    "\n",
    "#Remove age, indus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: Remove age and indus from model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's use 10-fold cross validation to choose our best model among the following candidates. Let's first add lstat**2 to our model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BostonData['lstat_2'] = BostonData['lstat']**2\n",
    "X1 = BostonData[['lstat']]\n",
    "X2 = BostonData[['lstat','lstat_2']]\n",
    "X3 = BostonData[['lstat','chas']]\n",
    "X4 = BostonData[['lstat','lstat_2','chas']] #'black' is highly correlated with lstat so cannot consider them simoltanously\n",
    "X5 = BostonData[['ptratio','chas']]\n",
    "X6 = BostonData[['ptratio','chas','black']]\n",
    "X7 = BostonData[['ptratio','black']]\n",
    "X8 = BostonData[['rm']]\n",
    "X9 = BostonData[['rm','chas']]\n",
    "X10 = BostonData[['rm','chas','black']]\n",
    "X11 = BostonData[['rm','black']]\n",
    "X12 = BostonData[['lstat','ptratio','rm']]  #model without that much interpretability\n",
    "X13 = BostonData[['lstat','lstat_2','ptratio','rm']]  #model without that much interpretability\n",
    "X14 = BostonData[['lstat','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "X15 = BostonData[['lstat','lstat_2','ptratio','rm','chas','black']]  #model without that much interpretability\n",
    "y = BostonData['medv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Use 10 fold cross-validation to decide on the model of your interest\n",
    "kf = cross_validation.KFold(len(BostonData), n_folds = 10, shuffle=True)\n",
    "overall_scores = []\n",
    "for x in [X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15]:\n",
    "    list_count = []\n",
    "    for train_index, test_index in kf:\n",
    "        lm.fit(x.iloc[train_index], y.iloc[train_index])\n",
    "        err_test = lm.predict(x.iloc[test_index])\n",
    "        current_score = metrics.mean_squared_error(err_test, y.iloc[test_index])\n",
    "        scores.append(current_score)\n",
    "    overall_scores.append({str(x.columns.values) : np.mean(scores)})\n",
    "    scores = []\n",
    "print overall_scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is interpretation - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for x in [X1, X2, X3, X4, X5, X6, X7, X8, X9, X10, X11, X12, X13, X14, X15]:\n",
    "    lm1 = smf.ols(formula='y ~ x', data=BostonData).fit()\n",
    "    #print(lm1.summary())\n",
    "    print x.columns.values\n",
    "    print(lm1.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If your goal is prediction - what model(s) are you going to use? Use  smf.ols  in \"statsmodels.formula.api as smf\" to test significancy of your coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [X12,X12,X14,X15]:\n",
    "    lm1 = smf.ols(formula='y ~ i', data=BostonData).fit()\n",
    "    print(lm1.summary())\n",
    "    \n",
    "    #All of our models are highly significant, so we use model 15. It generates the least CV-MSE. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
